{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "<span style=\"color:red\">**Insert Project Token (us the menu in the top right corner). If you don't complete this step, you will get an error in write_data_by_product_line()**</span>"}, {"metadata": {}, "cell_type": "markdown", "source": "# Use Databand SDK with Spark data pipelines\n**This notebook provides sample code for using Databand SDK**"}, {"metadata": {}, "cell_type": "code", "source": "# Run once to install dbnd library\n!pip install dbnd-spark", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Import Databand libraries\nfrom dbnd import dbnd_tracking, task, dataset_op_logger", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<span style=\"color:red\">**Make sure to replace code as documented in comments.**</span>"}, {"metadata": {}, "cell_type": "code", "source": "# Global variables\n\n# TODO: Update url and token\n\ndataband_url = 'insert_url'\ndataband_access_token = 'insert_token'\n\n# Data used in this pipeline\nRETAIL_FILE = \"https://raw.githubusercontent.com/elenalowery/data-samples/main/Retail_Products_and_Customers.csv\"\n\n# TODO: Provide a unique suffix that will be added to various assets tracked in Databand. We use this approach because\n# in a workshop many users are running the same sample pipelines. For example '_mi'\nunique_suffix = '_mi'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "@task\ndef read_raw_data():\n    \n    import ibmos2spark, os\n    from pyspark.sql import SparkSession\n    \n    \n    # TODO: Replace code in this section with the code you generate with Insert to Code (for the Retail Products and Customers file)\n    # The easiest way to accomplish this task is to insert a cell above and generate code in that cell\n    \n    import ibmos2spark, os\n    # @hidden_cell\n    credentials = {\n        'endpoint': 'https://s3.private.us.cloud-object-storage.appdomain.cloud',\n        'service_id': 'iam-ServiceId-8f035f08-7edb-4e66-b1f4-8c8ff3c2b837',\n        'iam_service_endpoint': 'https://iam.cloud.ibm.com/oidc/token',\n        'api_key': '4Ink9MxVC7S1QEhnzxfqljmw9WtilM6bSxBJesS'\n    }\n\n    configuration_name = 'os_1e498447d5f74cd6b90b92b35bb6514e_configs'\n    cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n\n    from pyspark.sql import SparkSession\n    spark = SparkSession.builder.getOrCreate()\n    retailData = spark.read\\\n      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n      .option('header', 'true')\\\n      .load(cos.url('Retail_Products_and_Customers.csv', 'datapipelines-donotdelete-pr-wb8fasnklfoldc'))\n    retailData.take(5)\n    \n    # End Replace section\n    \n    # Log the data read\n    with dataset_op_logger(\"CPDaaS://Weekly_Sales/Retail_Products_and_Customers.csv\", \"read\", with_schema=True, with_preview=True) as logger:\n        logger.set(data=retailData)\n    \n    return retailData", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "@task\ndef filter_data(rawData):\n    \n    # Drop a few columns\n    filteredRetailData = rawData.drop('Buy','PROFESSION','EDUCATION')\n    \n    with dataset_op_logger(\"script://WeeklySales/Filtered_spark_df\", \"read\", with_schema=True, with_preview=True) as logger:\n        logger.set(data=filteredRetailData)\n    \n    return filteredRetailData", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "@task\ndef write_data_by_product_line(filteredData):\n    \n    from pyspark.sql.functions import col\n\n    # Select any product line - we will write it to a separate file\n    campingEquipment = filteredData.filter(col(\"Product line\")==\"Camping Equipment\")\n    \n    # Log the filtered data read\n    with dataset_op_logger(\"CPDaaS://Weekly_Sales/Camping_Equipment.csv\", \"write\", with_schema=True, with_preview=True) as logger:\n        logger.set(data=campingEquipment)\n    \n    \n    # Workaround - Spark dataframe write fails when using project lib, that's why we are converting it to pandas\n    tempPandas = campingEquipment.toPandas()\n    # Write the csv file\n    project.save_data(\"Camping_Equipment.csv\", tempPandas.to_csv(index=False), overwrite=True)\n    \n    # If the issue is fixed, then the code will look like this\n    #project.save_data(\"CampingEquipment_spark.csv\", campingEquipment.write.csv(\"CampingEquipment_spark.csv\"), overwrite=True)\n             \n    \n    # Select any product line\n    golfEquipment = filteredData.filter(col(\"Product line\")==\"Golf Equipment\")\n    \n    # Log the filtered data read\n    with dataset_op_logger(\"CPDaaS://Weekly_Sales/Golf_Equipment.csv\", \"write\", with_schema=True, with_preview=True) as logger:\n        logger.set(data=golfEquipment)\n    \n    \n    # Workaround - Spark dataframe write fails when using project lib, that's why we are converting it to pandas\n    tempPandas = campingEquipment.toPandas()\n    # Write the csv file\n    project.save_data(\"Golf_Equipment.csv\", tempPandas.to_csv(index=False), overwrite=True)\n    \n    # If the issue is fixed, then the code will look like this\n    #project.save_data(\"GolfEquipment_spark.csv\", campingEquipment.write.csv(\"GolfEquipment_spark.csv\"), overwrite=True)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Call and track all steps in a pipeline\n\ndef prepareRetailData():\n    \n    with dbnd_tracking(\n            conf={\n                \"core\": {\n                    \"databand_url\": databand_url,\n                    \"databand_access_token\": databand_access_token,\n                }\n            },\n            job_name = \"prepare_sales_data_spark\",\n            run_name = \"weekly\",\n            project_name = \"Retail Analytics\",\n    ):\n    \n        # Call the step job - read data\n        rawData = read_raw_data()\n\n        # Filter data\n        filteredData = filter_data(rawData)\n\n        # Write data by product line\n        write_data_by_product_line(filteredData)\n\n        print(\"Finished running the pipeline\")\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Invoke the main function\nprepareRetailData()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python39", "display_name": "Python 3.9 with Spark", "language": "python3"}, "language_info": {"name": "python", "version": "3.9.12", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}