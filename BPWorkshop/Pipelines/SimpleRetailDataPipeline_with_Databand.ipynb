{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "<span style=\"color:red\">**Insert Project Token (us the menu in the top right corner). If you don't complete this step, you will get an error in write_data_by_product_line()**</span>"}, {"metadata": {}, "cell_type": "markdown", "source": "# Simple Retail Pipeline with Databand SKD"}, {"metadata": {}, "cell_type": "code", "source": "# Install databand - run once\n!pip install databand", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Import databand libraries\nfrom dbnd import dbnd_tracking, task, dataset_op_logger", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Run once - optional if you want to understand the data\n# !pip install pandas-profiling", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Global variables\n\ndataband_url = 'insert_url'\ndataband_access_token = 'insert_token'\n\n# Data used in this pipeline\nRETAIL_FILE = \"https://raw.githubusercontent.com/elenalowery/data-samples/main/Retail_Products_and_Customers.csv\"\n\n# Provide a unique suffix that will be added to various assets tracked in Databand. We use this approach because\n# in a workshop many users are running the same sample pipelines. For example '_mi'\nunique_suffix = '_mi'\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "@task\ndef read_raw_data():\n    \n    import pandas as pd\n    \n    url = 'https://raw.githubusercontent.com/elenalowery/data-samples/main/Retail_Products_and_Customers.csv'\n\n    retailData = pd.read_csv(RETAIL_FILE)\n    \n    # Log the data read\n\n    # Unique name for logging\n    unique_file_name = RETAIL_FILE + unique_suffix\n\n    # Log the data read\n    with dataset_op_logger(unique_file_name,\"read\",with_schema=True,with_preview=True,with_stats=True,with_histograms=True,) as logger:\n        retailData = pd.read_csv(RETAIL_FILE)\n        logger.set(data=retailData)\n    \n    return retailData", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "@task\ndef filter_data(rawData):\n    \n    unique_file_name = 'script://Weekly_Sales/Filtered_df' + unique_suffix\n\n    # Drop a few columns\n    filteredRetailData = rawData.drop(['Buy', 'PROFESSION', 'EDUCATION'], axis=1)\n\n    with dataset_op_logger(unique_file_name, \"read\", with_schema=True, with_preview=True) as logger:\n        logger.set(data=filteredRetailData)\n    \n    return filteredRetailData", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "@task\ndef write_data_by_product_line(filteredData):\n    \n    unique_file_name_1 = 'local://Weekly_Sales/Camping_Equipment.csv' + unique_suffix\n    unique_file_name_2 = 'local://Weekly_Sales/Golf_Equipment.csv' + unique_suffix\n\n    # Select any product line - we will write it to a separate file\n    campingEquipment = filteredData.loc[filteredData['Product line'] == 'Camping Equipment']\n\n    # Log writing the Camping Equipment csv\n    with dataset_op_logger(unique_file_name_1, \"write\", with_schema=True,with_preview=True) as logger:\n        # Write the csv file\n        project.save_data(\"CampingEquipment.csv\", campingEquipment.to_csv(index=False), overwrite=True)\n        logger.set(data=campingEquipment)\n\n    # Select any product line\n    golfEquipment = filteredData.loc[filteredData['Product line'] == 'Golf Equipment']\n\n    # Log the filtered data read\n    with dataset_op_logger(unique_file_name_2, \"write\", with_schema=True,with_preview=True) as logger:\n        # Write the csv file\n        project.save_data(\"GolfEquipment.csv\", golfEquipment.to_csv(index=False), overwrite=True)\n        logger.set(data=golfEquipment)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Call and track all steps in a pipeline\n\n# TODO: \n# - Update databand URL and token\n# - Update project and job name (add your initials)\n\ndef prepare_retail_data():\n    \n    with dbnd_tracking(\n            conf={\n                \"core\": {\n                    \"databand_url\": databand_url,\n                    \"databand_access_token\": databand_access_token,\n                }\n            },\n            job_name=\"prepare_sales_data\" + unique_suffix,\n            run_name=\"weekly\",\n            project_name=\"Retail Analytics\" + unique_suffix,\n    ):\n        \n        # Call the step job - read data\n        rawData = read_raw_data()\n\n        # Filter data\n        filteredData = filter_data(rawData)\n\n        # Write data by product line\n        write_data_by_product_line(filteredData)\n\n        print(\"Finished running the pipeline\")\n\n\n# Invoke the main function\nprepare_retail_data()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Optional if you want to understand the data\n#import pandas_profiling\n#retailData.profile_report()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Optional if you want to understand the data\n#retailData.columns", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Optional if you want to understand the data\n# Unique values in Product Line column\n# print(retailData['Product line'].unique())", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.9", "language": "python"}, "language_info": {"name": "python", "version": "3.9.12", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}