{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "<span style=\"color:red\">**Insert Project Token (us the menu in the top right corner). If you don't complete this step, you will get an error in write_data_by_state()**</span>"}, {"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "## Lineage Pipeline 2"}, {"metadata": {}, "cell_type": "code", "source": "# Run once to install the Databand library\n!pip install databand", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Import pandas and databand libraries\nimport pandas as pd\nfrom dbnd import dbnd_tracking, task, dataset_op_logger", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Global variables\n\ndataband_url = 'insert_url'\ndataband_access_token = 'insert_token'\n\n# Data used in this pipeline\nRETAIL_FILE = \"https://raw.githubusercontent.com/elenalowery/data-samples/main/Retail_Products_and_Customers.csv\"\n\n# Provide a unique suffix that will be added to various assets tracked in Databand. We use this approach because\n# in a workshop many users are running the same sample pipelines. For example '_mi'\nunique_suffix = '_mi'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "@task\ndef read_sales_data():\n    \n    url = 'https://raw.githubusercontent.com/elenalowery/data-samples/main/Camping_Equipment.csv'\n    \n    retailData = pd.read_csv(url)\n    \n    # Log the data read\n    with dataset_op_logger(\"local://Weekly_Sales/Camping_Equipment.csv\", \"read\", with_schema=True, with_preview=True) as logger:\n        logger.set(data=retailData)\n    \n    return retailData\n\n    # Unique name for logging - this matches the name used in LineagePipeline1\n    unique_file_name = 'local://Weekly_Sales/Camping_Equipment.csv' + unique_suffix\n\n    # Log the data read\n    with dataset_op_logger(unique_file_name, \"read\", with_schema=True, with_preview=True, with_stats=True,\n                           with_histograms=True, ) as logger:\n        retailData = pd.read_csv(INPUT_FILE)\n        logger.set(data=retailData)\n\n    return retailData", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "@task\ndef write_data_by_state(salesData):\n\n    # Select any product line - we will write it to a separate file\n    salesByState = salesData.loc[salesData['State'] == 'Arizona']\n    \n    # Log the filtered data read\n    with dataset_op_logger(\"local://Weekly_Sales/Arizona_Camping_Equipment.csv\", \"write\", with_schema=True, with_preview=True) as logger:\n        logger.set(data=salesByState)\n        \n    project.save_data(\"Arizona_Camping_Equipment.csv\", salesByState.to_csv(index=False), overwrite=True)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Call and track all steps in a pipeline\n\n# TODO: \n# - Update databand URL and token\n# - Update project and job name (add your initials)\n\ndef prepare_retail_data():\n\n    with dbnd_tracking(\n            conf={\n                \"core\": {\n                    \"databand_url\": databand_url,\n                    \"databand_access_token\": databand_access_token,\n                }\n            },\n            job_name=\"lineage_pipeline_2_notebook\",\n            run_name=\"weekly\",\n            project_name=\"Lineage Test\",\n    ):\n\n        #Call the step job - read data\n        salesData = read_sales_data()\n\n        # Write data by product line\n        write_data_by_state(salesData)\n\n        print(\"Finished running the pipeline\")\n\n# Invoke the main function\nprepare_retail_data()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.9", "language": "python"}, "language_info": {"name": "python", "version": "3.9.12", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}